{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f43594-ec78-4455-9898-da19335e3363",
   "metadata": {},
   "source": [
    "# Наумкин Владимир, С01-119."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6a5c6-71a7-4bd3-ae96-0c7597b31509",
   "metadata": {},
   "source": [
    "## Задача 3. Модель автокодировщика."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a5c3eb-6b17-4def-8880-b44109e68ab4",
   "metadata": {},
   "source": [
    "### Подключим библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aac232d-71fa-4690-99e9-48678d136fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from prettytable import PrettyTable\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265650f1-8b65-4b0d-8a03-8851e20a1c5c",
   "metadata": {},
   "source": [
    "### Уберём предупреждения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcbecaf5-ff9d-4fc9-85ef-7cc8da55ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d3cc3-382d-4418-a553-5934ea0d2556",
   "metadata": {},
   "source": [
    "### Зададим устройство исполнения кода (вычисления провожу на своём ПК)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "622975dd-81c0-4ea8-bac2-752ab54ad7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b93c2ce-3a04-4fc2-8838-2329c2a1e019",
   "metadata": {},
   "source": [
    "### Функции для работы с датасетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0504342-09ad-4380-b937-dae186f260d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, word_to_ind, tokenizer):\n",
    "        self.word_to_ind = word_to_ind\n",
    "        self.tokenizer = tokenizer\n",
    "    def __call__(self, sentences, max_len = 10, strict_padding_to_max_len = True):\n",
    "        tokens = self.tokenizer.tokenize_sents(sentences)\n",
    "        if strict_padding_to_max_len == False: # Пытаемся уменьшить max_len \n",
    "            max_len = min(max_len, max(map(len, tokens))) # до длины самого длинного предложения.\n",
    "        def process_sentence(s, ma_l):\n",
    "            if len(s) < ma_l:\n",
    "                return ['[CLS]'] + s + ['[SEP]'] + ['[PAD]'] * (ma_l - len(s))\n",
    "            else:\n",
    "                return ['[CLS]'] + s[: ma_l] + ['[SEP]']\n",
    "        tokens = [process_sentence(sent, max_len) for sent in tokens]\n",
    "        idxs = [[self.word_to_ind.get(word, self.word_to_ind['[UNK]']) for word in sent] for sent in tokens]\n",
    "        return torch.tensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90b402b-9b9d-4d24-a535-f83201f22535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_dict(dataset, min_count = 1): # Не включаем в словарь редкие слова\n",
    "    temp = {}\n",
    "    for sent in tqdm(dataset.values[:, 1]):\n",
    "        for word in RegexpTokenizer('[a-zA-Z]+|[^\\w\\s]|\\d+').tokenize(sent):\n",
    "            # слова или знаки препинания (не буквы и цифры, пробелы и табы) или цифры\n",
    "            if word in temp:\n",
    "                temp[word] += 1\n",
    "            else:\n",
    "                temp[word] = 1\n",
    "    word2idx = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3}\n",
    "    idx2word = {0: '[PAD]', 1: '[UNK]', 2: '[CLS]', 3: '[SEP]'}\n",
    "    for elem, number in temp.items():\n",
    "        if number >= min_count and elem not in word2idx:\n",
    "            word2idx[elem] = len(word2idx)\n",
    "            idx2word[len(idx2word)] = elem\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646c460-1449-4d6a-a885-d9ffe9bc249f",
   "metadata": {},
   "source": [
    "### Функция проверки качества модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3683c37-01e1-49b7-9c5e-f5c8e7d70e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model(batch_size, dataset, model, loss_function, idx2word):\n",
    "    model.eval()\n",
    "    batch_generator = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "    test_loss = 0\n",
    "    for it, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "        x_batch = x_batch.to(model.device)\n",
    "        y_batch = y_batch.to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output = model(x_batch)\n",
    "        test_loss += loss_function(output.transpose(1,2), y_batch).cpu().item()*len(x_batch)\n",
    "    test_loss /= len(dataset)\n",
    "    print(f'loss: {test_loss}')\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    x, y = next(iter(dataloader))\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x)\n",
    "    one_x = x[0].cpu().numpy()\n",
    "    one_output = outputs[0].argmax(dim=-1).cpu().numpy()\n",
    "    words = [idx2word[idx] for idx in one_x]\n",
    "    pred_words = [idx2word[idx] for idx in one_output]\n",
    "    table = PrettyTable([\"Word\", \"Predict\"])\n",
    "    table.align[\"Word\"], table.align[\"Predict\"] = \"l\", \"l\"\n",
    "    for word, pred in zip(words, pred_words):\n",
    "        if word != idx2word[word2idx['[PAD]']]:\n",
    "            table.add_row([word, pred])\n",
    "    print(table)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d99e85-f929-4500-a2e8-0cfc34b684ee",
   "metadata": {},
   "source": [
    "### Код для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2429eddd-ae8e-4a09-8837-0721575a674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, x_batch, y_batch, optimizer, loss_function):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_batch.to(model.device))\n",
    "    loss = loss_function(output.transpose(1,2), y_batch.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aadafb3a-2067-4f76-9e41-a78e36e9f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_generator, model, loss_function, optimizer, callback = None):\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        batch_loss = train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function)\n",
    "        if callback is not None:\n",
    "            with torch.no_grad():\n",
    "                callback(model, batch_loss)\n",
    "        epoch_loss += batch_loss*len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "    return epoch_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b3dfbc-d5a5-4b09-b615-20ebcbac31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch, \n",
    "            batch_size, \n",
    "            dataset,\n",
    "            model, \n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            lr = 0.001,\n",
    "            callback = None):\n",
    "    optima = optimizer(model.parameters(), lr=lr)\n",
    "    iterations = tqdm(range(count_of_epoch), desc='epoch')\n",
    "    iterations.set_postfix({'train epoch loss': np.nan})\n",
    "    for it in iterations:\n",
    "        batch_generator = tqdm(\n",
    "            torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True), \n",
    "            leave=False, total=len(dataset)//batch_size+(len(dataset)%batch_size> 0))\n",
    "        epoch_loss = train_epoch(train_generator=batch_generator, \n",
    "                    model=model, \n",
    "                    loss_function=loss_function, \n",
    "                    optimizer=optima, \n",
    "                    callback=callback)\n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4356a4-ab24-455b-9fea-9c9482e6fd5e",
   "metadata": {},
   "source": [
    "### Отслеживание обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79ba4965-0bcd-44d1-954f-25636c8fd12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback():\n",
    "    def __init__(self, writer, dataset, loss_function, delimeter = 300, batch_size=64):\n",
    "        self.step = 0\n",
    "        self.writer = writer\n",
    "        self.delimeter = delimeter\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = dataset\n",
    "    def forward(self, model, loss):\n",
    "        model.eval()\n",
    "        self.step += 1\n",
    "        self.writer.add_scalar('LOSS/train', loss, self.step)\n",
    "        if self.step % self.delimeter == 0:\n",
    "            model.eval()\n",
    "            batch_generator = torch.utils.data.DataLoader(dataset=self.dataset, batch_size=self.batch_size)\n",
    "            test_loss = 0\n",
    "            for it, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "                x_batch = x_batch.to(model.device)\n",
    "                y_batch = y_batch.to(model.device)\n",
    "                output = model(x_batch)\n",
    "                test_loss += self.loss_function(output.transpose(1,2), y_batch).cpu().item()*len(x_batch)\n",
    "            test_loss /= len(self.dataset)\n",
    "            print(f'\\t\\tstep={self.step}, train_loss={loss}, val_loss={test_loss}')\n",
    "            self.writer.add_scalar('LOSS/test', test_loss, self.step)\n",
    "    def __call__(self, model, loss):\n",
    "        return self.forward(model, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7640d10-be9a-41a8-b6b2-c105b341c2b4",
   "metadata": {},
   "source": [
    "## Модель автокодировщика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4715740c-2126-43d7-b130-e30519638024",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    def __init__(self, vocab_dim, emb_dim, latent_dim, num_layers = 3, dropout = 0, batch_norm = False):\n",
    "        super(type(self), self).__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_dim, emb_dim)\n",
    "        self.lstm = torch.nn.LSTM(emb_dim, latent_dim, num_layers, dropout = dropout, batch_first = True)\n",
    "        if batch_norm:\n",
    "            self.batch_norm = torch.nn.BatchNorm1d(latent_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "    def forward(self, x):\n",
    "        out = self.emb(x)\n",
    "        _, (h, c) = self.lstm(out)\n",
    "        if self.batch_norm is not None:\n",
    "            out = self.batch_norm(out.transpose(1,2)).transpose(1,2)\n",
    "        out = torch.cat([h, c], dim=-1).transpose(0, 1)[:, -1, :] # cat => 2 * latent_dim\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "349cbe6b-086a-4d34-9556-620ad1073791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    def __init__(self, vocab_dim, latent_dim, emb_dim, hidden_dim, num_layers = 3, dropout = 0, batch_norm = False):\n",
    "        super(type(self), self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.h0 = torch.nn.Linear(latent_dim, hidden_dim)\n",
    "        self.c0 = torch.nn.Linear(latent_dim, hidden_dim)\n",
    "        self.emb = torch.nn.Embedding(1, emb_dim)\n",
    "        self.lstm = torch.nn.LSTM(emb_dim, hidden_dim, num_layers, dropout = dropout, batch_first = True)\n",
    "        if batch_norm:\n",
    "            self.batch_norm = torch.nn.BatchNorm1d(emb_dim)\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "        self.linear = torch.nn.Linear(hidden_dim, vocab_dim)\n",
    "    def forward(self, latent_vector):\n",
    "        h = self.h0(latent_vector).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        c = self.c0(latent_vector).unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "        emb = self.emb(torch.zeros(len(latent_vector), 1).long())\n",
    "        logits = []\n",
    "        for i in range(12): # в Tokenizer по умолчанию max_len = 10, но ещё + 2 токена начала и конца\n",
    "            out, (h, c) = self.lstm(emb, (h, c))\n",
    "            if self.batch_norm is not None:\n",
    "                out = self.batch_norm(out.transpose(1,2)).transpose(1,2)\n",
    "            logits.append(out[:,-1,:])\n",
    "        out = torch.stack(logits, 1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e6c08bb-05cc-4f0b-a41b-4645d304ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "    def __init__(self, vocab_dim, emb_dim, latent_dim, hidden_dim, num_layers = 3, dropout = 0, batch_norm = False):\n",
    "        super(type(self), self).__init__()\n",
    "        self.encoder = Encoder(vocab_dim, emb_dim, latent_dim, num_layers, dropout, batch_norm)\n",
    "        self.decoder = Decoder(vocab_dim, 2 * latent_dim, emb_dim, hidden_dim, num_layers, dropout, batch_norm)\n",
    "        # 2 * latent_dim - смотри forward в Encoder\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd7e61-1407-47a3-8bfd-54e7e6a4f087",
   "metadata": {},
   "source": [
    "### Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66580b12-2fc3-444e-9b6c-eef3ad039c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('twitter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15f0c04a-ad79-4018-bf7f-a627782a7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset[['tag', 'message']].notnull().all(1)]\n",
    "dataset = dataset.sample(100000, random_state = 777)\n",
    "dataset_train, dataset_test = train_test_split(dataset, test_size = 0.2, random_state = 777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb33e737-cf9d-4b5b-a411-9a7baea608b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34a80ca4a054f47bdc2ddca6b1d142e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word2idx, idx2word = word_dict(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac0e269-5f50-4367-b7da-16c0e00de46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(word2idx, RegexpTokenizer('[a-zA-Z]+|[^\\w\\s]|\\d+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89a42d0a-2a86-40d1-bca3-ed55d5db4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sent = tokenizer(dataset_train.values[:, 1])\n",
    "test_data_sent = tokenizer(dataset_test.values[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b0f3298-f2fa-4102-b683-a7c27d14a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# переводим в формат PyTorch данные, причём т.к. у нас автокодировщик, то выход в идеале совпадает со входом\n",
    "dataset_train_pt = TensorDataset(train_data_sent, train_data_sent)\n",
    "dataset_test_pt = TensorDataset(test_data_sent, test_data_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641a4f6-0b38-4ccd-b921-31a731e728a8",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fcc32f0-95ca-4e20-bd12-e8908cb107d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcdd547-6fcb-4a30-b050-a663d37efba1",
   "metadata": {},
   "source": [
    "Перебираемые параметры модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff8decc6-c20a-457b-a94c-4b09591401d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_params = [10, 25, 50]\n",
    "num_layers_params = [3, 5, 7]\n",
    "dropout_params = [0, 0.25, 0.5]\n",
    "batch_norm_params = [False, True]\n",
    "min_count_params = [1, 3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f381321-5f51-49f0-b9cc-aa1e39e4b965",
   "metadata": {},
   "source": [
    "Разный размер слоя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c61ddbe-4fb7-4134-bb9c-358850714e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim = 10\n",
      "loss: 11.426295571899415\n",
      "+-----------+---------------+\n",
      "| Word      | Predict       |\n",
      "+-----------+---------------+\n",
      "| [CLS]     | sorryy        |\n",
      "| Is        | Huggg         |\n",
      "| listening | JackieKessler |\n",
      "| to        | JackieKessler |\n",
      "| echelon   | JulietWeybret |\n",
      "| .         | JulietWeybret |\n",
      "| It        | JulietWeybret |\n",
      "| '         | JulietWeybret |\n",
      "| s         | JulietWeybret |\n",
      "| been      | JulietWeybret |\n",
      "| my        | JulietWeybret |\n",
      "| [SEP]     | JulietWeybret |\n",
      "+-----------+---------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135be0710809447eadce88fe8fb39ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.401067733764648, val_loss=7.243672047424316\n",
      "\t\tstep=600, train_loss=6.495371341705322, val_loss=6.445184295654297\n",
      "\t\tstep=900, train_loss=6.391746997833252, val_loss=6.364573110198974\n",
      "\t\tstep=1200, train_loss=6.356290340423584, val_loss=6.3169028869628905\n",
      "loss: 6.302846558380127\n",
      "+------------+---------+\n",
      "| Word       | Predict |\n",
      "+------------+---------+\n",
      "| [CLS]      | [CLS]   |\n",
      "| I          | [SEP]   |\n",
      "| think      | [SEP]   |\n",
      "| I          | [SEP]   |\n",
      "| '          | [SEP]   |\n",
      "| m          | [SEP]   |\n",
      "| going      | [SEP]   |\n",
      "| to         | [SEP]   |\n",
      "| miss       | [SEP]   |\n",
      "| #          | [SEP]   |\n",
      "| masterchef | [SEP]   |\n",
      "| [SEP]      | [SEP]   |\n",
      "+------------+---------+\n",
      "dim = 25\n",
      "loss: 11.463343186950684\n",
      "+--------+-----------------+\n",
      "| Word   | Predict         |\n",
      "+--------+-----------------+\n",
      "| [CLS]  | shortstackhater |\n",
      "| he     | MacFUSE         |\n",
      "| looks  | MacFUSE         |\n",
      "| a      | MacFUSE         |\n",
      "| bit    | MacFUSE         |\n",
      "| stoned | MacFUSE         |\n",
      "| .      | MacFUSE         |\n",
      "| .      | MacFUSE         |\n",
      "| .      | MacFUSE         |\n",
      "| but    | MacFUSE         |\n",
      "| still  | MacFUSE         |\n",
      "| [SEP]  | MacFUSE         |\n",
      "+--------+-----------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc35c0cdb9e64bec9a42e429764d22b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=6.339521884918213, val_loss=6.264262136077881\n",
      "\t\tstep=600, train_loss=6.075460433959961, val_loss=6.059688710021972\n",
      "\t\tstep=900, train_loss=5.932245254516602, val_loss=5.87548848953247\n",
      "\t\tstep=1200, train_loss=5.700625896453857, val_loss=5.753637548065186\n",
      "loss: 5.736734986877441\n",
      "+----------+---------+\n",
      "| Word     | Predict |\n",
      "+----------+---------+\n",
      "| [CLS]    | [CLS]   |\n",
      "| @        | @       |\n",
      "| dhewlett | [CLS]   |\n",
      "| I        | I       |\n",
      "| '        | .       |\n",
      "| m        | .       |\n",
      "| still    | .       |\n",
      "| trying   | .       |\n",
      "| to       | .       |\n",
      "| see      | .       |\n",
      "| Star     | [SEP]   |\n",
      "| [SEP]    | [SEP]   |\n",
      "+----------+---------+\n",
      "dim = 50\n",
      "loss: 11.462046812438965\n",
      "+-------------+---------------+\n",
      "| Word        | Predict       |\n",
      "+-------------+---------------+\n",
      "| [CLS]       | SarahAnnGreen |\n",
      "| @           | McFlyNews     |\n",
      "| DontTrustMe | McFlyNews     |\n",
      "| 49          | McFlyNews     |\n",
      "| thankyou    | McFlyNews     |\n",
      "| [SEP]       | McFlyNews     |\n",
      "+-------------+---------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc63793014e44b189070c35c2905a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=5.983788967132568, val_loss=6.018139950561523\n",
      "\t\tstep=600, train_loss=5.899569511413574, val_loss=5.743125173187256\n",
      "\t\tstep=900, train_loss=5.682153224945068, val_loss=5.62994130859375\n",
      "\t\tstep=1200, train_loss=5.431457042694092, val_loss=5.5370668601989745\n",
      "loss: 5.5228437919616695\n",
      "+-------+---------+\n",
      "| Word  | Predict |\n",
      "+-------+---------+\n",
      "| [CLS] | [CLS]   |\n",
      "| In    | @       |\n",
      "| bed   | '       |\n",
      "| .     | '       |\n",
      "| I     | I       |\n",
      "| know  | I       |\n",
      "| I     | I       |\n",
      "| '     | I       |\n",
      "| m     | I       |\n",
      "| going | '       |\n",
      "| to    | .       |\n",
      "| [SEP] | [SEP]   |\n",
      "+-------+---------+\n"
     ]
    }
   ],
   "source": [
    "for dim in dim_params:\n",
    "    print(f'dim = {dim}')\n",
    "    model = Autoencoder(vocab_dim=len(word2idx), emb_dim=dim, latent_dim=dim, hidden_dim=dim)\n",
    "    model.to(device)\n",
    "    writer = SummaryWriter(log_dir=f'tensorboard3/dim_{dim}')\n",
    "    call = callback(writer, dataset_test_pt, loss_function)\n",
    "    check_model(64, dataset_test_pt, model, loss_function, idx2word)\n",
    "    trainer(count_of_epoch=1,\n",
    "            batch_size=64,\n",
    "            dataset=dataset_train_pt,\n",
    "            model=model,\n",
    "            loss_function=loss_function,\n",
    "            optimizer = optimizer,\n",
    "            lr=0.001,\n",
    "            callback = call)\n",
    "    check_model(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1c3f3-6090-4c2a-862b-bac31a560fa3",
   "metadata": {},
   "source": [
    "Разное число слоёв:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8075ab2-b425-4da0-8cb1-8a93ef8b7057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers = 3\n",
      "loss: 11.465850242614746\n",
      "+---------+---------+\n",
      "| Word    | Predict |\n",
      "+---------+---------+\n",
      "| [CLS]   | Deejay  |\n",
      "| @       | nevah   |\n",
      "| [UNK]   | nevah   |\n",
      "| Can     | bikers  |\n",
      "| '       | bikers  |\n",
      "| t       | bikers  |\n",
      "| fricken | bikers  |\n",
      "| wait    | bikers  |\n",
      "| !       | bikers  |\n",
      "| Still   | bikers  |\n",
      "| sad     | bikers  |\n",
      "| [SEP]   | bikers  |\n",
      "+---------+---------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3dcfd0937446efaa041600c7dfebe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.324864387512207, val_loss=7.288043412017823\n",
      "\t\tstep=600, train_loss=6.365283012390137, val_loss=6.389882967376709\n",
      "\t\tstep=900, train_loss=6.40216588973999, val_loss=6.285347200012207\n",
      "\t\tstep=1200, train_loss=6.322598457336426, val_loss=6.236143389892578\n",
      "loss: 6.216730666351318\n",
      "+--------+---------+\n",
      "| Word   | Predict |\n",
      "+--------+---------+\n",
      "| [CLS]  | [CLS]   |\n",
      "| Sad    | [SEP]   |\n",
      "| !      | [CLS]   |\n",
      "| Chris  | [SEP]   |\n",
      "| has    | [SEP]   |\n",
      "| to     | [SEP]   |\n",
      "| work   | [SEP]   |\n",
      "| on     | [SEP]   |\n",
      "| Easter | [SEP]   |\n",
      "| [SEP]  | [SEP]   |\n",
      "+--------+---------+\n",
      "num_layers = 5\n",
      "loss: 11.458915177917481\n",
      "+---------+---------------+\n",
      "| Word    | Predict       |\n",
      "+---------+---------------+\n",
      "| [CLS]   | anilmujagic   |\n",
      "| @       | 3133          |\n",
      "| [UNK]   | withgoodworks |\n",
      "| sorry   | withgoodworks |\n",
      "| I       | withgoodworks |\n",
      "| '       | withgoodworks |\n",
      "| m       | withgoodworks |\n",
      "| missing | withgoodworks |\n",
      "| it      | withgoodworks |\n",
      "| [SEP]   | withgoodworks |\n",
      "+---------+---------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148c2d5d8b464f7e92cce06253988745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.110495567321777, val_loss=7.119984931182861\n",
      "\t\tstep=600, train_loss=6.646178722381592, val_loss=6.5703259887695316\n",
      "\t\tstep=900, train_loss=6.695794582366943, val_loss=6.554778318023682\n",
      "\t\tstep=1200, train_loss=6.4009108543396, val_loss=6.4913670951843265\n",
      "loss: 6.473345589447021\n",
      "+---------+---------+\n",
      "| Word    | Predict |\n",
      "+---------+---------+\n",
      "| [CLS]   | [CLS]   |\n",
      "| Cant    | [SEP]   |\n",
      "| sleep   | [SEP]   |\n",
      "| =       | [SEP]   |\n",
      "| (       | [SEP]   |\n",
      "| Have    | [SEP]   |\n",
      "| a       | [SEP]   |\n",
      "| lot     | [SEP]   |\n",
      "| of      | [SEP]   |\n",
      "| packing | [SEP]   |\n",
      "| to      | [CLS]   |\n",
      "| [SEP]   | [CLS]   |\n",
      "+---------+---------+\n",
      "num_layers = 7\n",
      "loss: 11.470828549194335\n",
      "+-------+----------+\n",
      "| Word  | Predict  |\n",
      "+-------+----------+\n",
      "| [CLS] | coons    |\n",
      "| @     | calamari |\n",
      "| [UNK] | calamari |\n",
      "| was   | calamari |\n",
      "| gonna | calamari |\n",
      "| DM    | calamari |\n",
      "| you   | calamari |\n",
      "| but   | calamari |\n",
      "| it    | calamari |\n",
      "| says  | calamari |\n",
      "| you   | calamari |\n",
      "| [SEP] | calamari |\n",
      "+-------+----------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2319501b7af94d9f82cf89afbb133af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.208164691925049, val_loss=7.335866539001465\n",
      "\t\tstep=600, train_loss=6.451229095458984, val_loss=6.39777795791626\n",
      "\t\tstep=900, train_loss=6.33244514465332, val_loss=6.2405818359375\n",
      "\t\tstep=1200, train_loss=6.147186279296875, val_loss=6.167121617889404\n",
      "loss: 6.1477489791870115\n",
      "+-------+---------+\n",
      "| Word  | Predict |\n",
      "+-------+---------+\n",
      "| [CLS] | [CLS]   |\n",
      "| @     | @       |\n",
      "| [UNK] | @       |\n",
      "| i     | [SEP]   |\n",
      "| have  | [SEP]   |\n",
      "| heard | [SEP]   |\n",
      "| of    | [SEP]   |\n",
      "| that  | [SEP]   |\n",
      "| movie | [SEP]   |\n",
      "| .     | [SEP]   |\n",
      "| just  | [SEP]   |\n",
      "| [SEP] | [SEP]   |\n",
      "+-------+---------+\n"
     ]
    }
   ],
   "source": [
    "for num_layers in num_layers_params:\n",
    "    print(f'num_layers = {num_layers}')\n",
    "    model = Autoencoder(vocab_dim=len(word2idx), emb_dim=10, latent_dim=10, hidden_dim=10, num_layers=num_layers)\n",
    "    model.to(device)\n",
    "    writer = SummaryWriter(log_dir=f'tensorboard3/num_layers_{num_layers}')\n",
    "    call = callback(writer, dataset_test_pt, loss_function)\n",
    "    check_model(64, dataset_test_pt, model, loss_function, idx2word)\n",
    "    trainer(count_of_epoch=1,\n",
    "            batch_size=64,\n",
    "            dataset=dataset_train_pt,\n",
    "            model=model,\n",
    "            loss_function=loss_function,\n",
    "            optimizer = optimizer,\n",
    "            lr=0.001,\n",
    "            callback = call)\n",
    "    check_model(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb127e-d94d-4e82-8c6b-e25aeaf2fc37",
   "metadata": {},
   "source": [
    "Зависимость от dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9001b471-599f-4448-81a7-ebe0fb49e9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout = 0\n",
      "loss: 11.52413955078125\n",
      "+---------+------------+\n",
      "| Word    | Predict    |\n",
      "+---------+------------+\n",
      "| [CLS]   | IchigoNoiZ |\n",
      "| playing | IchigoNoiZ |\n",
      "| intense | IchigoNoiZ |\n",
      "| bingo   | IchigoNoiZ |\n",
      "| in      | migre      |\n",
      "| sunrise | migre      |\n",
      "| .       | migre      |\n",
      "| .       | migre      |\n",
      "| .       | migre      |\n",
      "| wow     | migre      |\n",
      "| [SEP]   | migre      |\n",
      "+---------+------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da685444ec424f44a9ecd07c9b7d7370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.416171550750732, val_loss=7.357627681732177\n",
      "\t\tstep=600, train_loss=6.580418109893799, val_loss=6.41712360534668\n",
      "\t\tstep=900, train_loss=6.361220359802246, val_loss=6.281792028045654\n",
      "\t\tstep=1200, train_loss=6.123701095581055, val_loss=6.179844000244141\n",
      "loss: 6.167657291412353\n",
      "+-----------+---------+\n",
      "| Word      | Predict |\n",
      "+-----------+---------+\n",
      "| [CLS]     | [CLS]   |\n",
      "| is        | @       |\n",
      "| uploading | @       |\n",
      "| pics      | [SEP]   |\n",
      "| on        | [SEP]   |\n",
      "| her       | [SEP]   |\n",
      "| facebook  | [SEP]   |\n",
      "| profile   | [SEP]   |\n",
      "| .         | [SEP]   |\n",
      "| .         | [SEP]   |\n",
      "| .         | [SEP]   |\n",
      "| [SEP]     | [SEP]   |\n",
      "+-----------+---------+\n",
      "dropout = 0.25\n",
      "loss: 11.480394509887695\n",
      "+-----------+-------------+\n",
      "| Word      | Predict     |\n",
      "+-----------+-------------+\n",
      "| [CLS]     | oddly       |\n",
      "| i         | tristankent |\n",
      "| wish      | lordy       |\n",
      "| i         | lordy       |\n",
      "| were      | lordy       |\n",
      "| going     | lordy       |\n",
      "| to        | lordy       |\n",
      "| bamboozle | ashlei      |\n",
      "| tomorrow  | ashlei      |\n",
      "| to        | ashlei      |\n",
      "| see       | ashlei      |\n",
      "| [SEP]     | ashlei      |\n",
      "+-----------+-------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce9862bf68841ea8ebc4dee59f45876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.284395217895508, val_loss=7.219936313629151\n",
      "\t\tstep=600, train_loss=6.483773231506348, val_loss=6.426385845184326\n",
      "\t\tstep=900, train_loss=6.156592845916748, val_loss=6.315860888671875\n",
      "\t\tstep=1200, train_loss=6.134279251098633, val_loss=6.173957640075684\n",
      "loss: 6.154406372833252\n",
      "+---------+---------+\n",
      "| Word    | Predict |\n",
      "+---------+---------+\n",
      "| [CLS]   | [CLS]   |\n",
      "| Is      | [SEP]   |\n",
      "| gonna   | [SEP]   |\n",
      "| fry     | [SEP]   |\n",
      "| some    | [SEP]   |\n",
      "| chicken | [SEP]   |\n",
      "| later   | [SEP]   |\n",
      "| 4       | [SEP]   |\n",
      "| me      | [SEP]   |\n",
      "| ,       | [SEP]   |\n",
      "| my      | [SEP]   |\n",
      "| [SEP]   | [SEP]   |\n",
      "+---------+---------+\n",
      "dropout = 0.5\n",
      "loss: 11.431422059631348\n",
      "+-----------+---------+\n",
      "| Word      | Predict |\n",
      "+-----------+---------+\n",
      "| [CLS]     | iyan    |\n",
      "| @         | godness |\n",
      "| [UNK]     | godness |\n",
      "| I         | godness |\n",
      "| can       | godness |\n",
      "| see       | godness |\n",
      "| them      | godness |\n",
      "| both      | godness |\n",
      "| switching | godness |\n",
      "| .         | Mick    |\n",
      "| but       | Mick    |\n",
      "| [SEP]     | Mick    |\n",
      "+-----------+---------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e1ef91ae114713b1830d21a860bddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.099998474121094, val_loss=7.1858977066040035\n",
      "\t\tstep=600, train_loss=6.4759416580200195, val_loss=6.423458170318604\n",
      "\t\tstep=900, train_loss=6.4295806884765625, val_loss=6.342418541717529\n",
      "\t\tstep=1200, train_loss=6.101335048675537, val_loss=6.257410876464844\n",
      "loss: 6.240466682434082\n",
      "+---------+---------+\n",
      "| Word    | Predict |\n",
      "+---------+---------+\n",
      "| [CLS]   | [CLS]   |\n",
      "| my      | [SEP]   |\n",
      "| girlies | [SEP]   |\n",
      "| @       | [SEP]   |\n",
      "| [UNK]   | [SEP]   |\n",
      "| &       | [SEP]   |\n",
      "| amp     | [SEP]   |\n",
      "| ;       | [SEP]   |\n",
      "| @       | [SEP]   |\n",
      "| [UNK]   | [SEP]   |\n",
      "| r       | [SEP]   |\n",
      "| [SEP]   | [SEP]   |\n",
      "+---------+---------+\n"
     ]
    }
   ],
   "source": [
    "for dropout in dropout_params:\n",
    "    print(f'dropout = {dropout}')\n",
    "    model = Autoencoder(vocab_dim=len(word2idx), emb_dim=10, latent_dim=10, hidden_dim=10, dropout=dropout)\n",
    "    model.to(device)\n",
    "    writer = SummaryWriter(log_dir=f'tensorboard3/dropout_{dropout}')\n",
    "    call = callback(writer, dataset_test_pt, loss_function)\n",
    "    check_model(64, dataset_test_pt, model, loss_function, idx2word)\n",
    "    trainer(count_of_epoch=1,\n",
    "            batch_size=64,\n",
    "            dataset=dataset_train_pt,\n",
    "            model=model,\n",
    "            loss_function=loss_function,\n",
    "            optimizer = optimizer,\n",
    "            lr=0.001,\n",
    "            callback = call)\n",
    "    check_model(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a856ce-3738-4637-b2ae-02ccc0705369",
   "metadata": {},
   "source": [
    "Добавление BatchNorm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2abae0f-2c90-424a-b71e-614d3308a3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_norm = False\n",
      "loss: 11.495204937744141\n",
      "+-----------+-----------------+\n",
      "| Word      | Predict         |\n",
      "+-----------+-----------------+\n",
      "| [CLS]     | BryonyRocks     |\n",
      "| @         | voluptuouspanic |\n",
      "| [UNK]     | voluptuouspanic |\n",
      "| Yes       | voluptuouspanic |\n",
      "| ,         | voluptuouspanic |\n",
      "| I         | voluptuouspanic |\n",
      "| agree     | voluptuouspanic |\n",
      "| .         | voluptuouspanic |\n",
      "| [UNK]     | voluptuouspanic |\n",
      "| difficult | voluptuouspanic |\n",
      "| to        | voluptuouspanic |\n",
      "| [SEP]     | voluptuouspanic |\n",
      "+-----------+-----------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a5ab54b3204268b672cd351fe9f399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.137426853179932, val_loss=7.25108508682251\n",
      "\t\tstep=600, train_loss=6.502966403961182, val_loss=6.437609629821777\n",
      "\t\tstep=900, train_loss=6.442091464996338, val_loss=6.343654668426514\n",
      "\t\tstep=1200, train_loss=6.122651100158691, val_loss=6.210512525939941\n",
      "loss: 6.188670066070556\n",
      "+-----------+---------+\n",
      "| Word      | Predict |\n",
      "+-----------+---------+\n",
      "| [CLS]     | [CLS]   |\n",
      "| According | [SEP]   |\n",
      "| to        | [SEP]   |\n",
      "| this      | [SEP]   |\n",
      "| article   | [SEP]   |\n",
      "| ,         | [SEP]   |\n",
      "| the       | [SEP]   |\n",
      "| Taco      | [SEP]   |\n",
      "| Bell      | [SEP]   |\n",
      "| [UNK]     | [SEP]   |\n",
      "| Sauce     | [SEP]   |\n",
      "| [SEP]     | [SEP]   |\n",
      "+-----------+---------+\n",
      "batch_norm = True\n",
      "loss: 11.481671836853028\n",
      "+-------+------------+\n",
      "| Word  | Predict    |\n",
      "+-------+------------+\n",
      "| [CLS] | Airy       |\n",
      "| mo    | Fb         |\n",
      "| ¿     | RJFlamingo |\n",
      "| o     | RJFlamingo |\n",
      "| no    | RJFlamingo |\n",
      "| [UNK] | RJFlamingo |\n",
      "| ¿     | RJFlamingo |\n",
      "| [UNK] | RJFlamingo |\n",
      "| &     | RJFlamingo |\n",
      "| quot  | RJFlamingo |\n",
      "| ;     | RJFlamingo |\n",
      "| [SEP] | RJFlamingo |\n",
      "+-------+------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba6ee5dd36347e5bbfe0194fd7cb1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=8.163247108459473, val_loss=10.113876968383789\n",
      "\t\tstep=600, train_loss=6.417255878448486, val_loss=15.904628247070313\n",
      "\t\tstep=900, train_loss=5.674330234527588, val_loss=19.049508041381834\n",
      "\t\tstep=1200, train_loss=5.843593120574951, val_loss=21.089601528930665\n",
      "loss: 24.01557347717285\n",
      "+---------+----------+\n",
      "| Word    | Predict  |\n",
      "+---------+----------+\n",
      "| [CLS]   | megashea |\n",
      "| Finally | slpknt   |\n",
      "| at      | slpknt   |\n",
      "| home    | '        |\n",
      "| with    | '        |\n",
      "| my      | '        |\n",
      "| honey   | '        |\n",
      "| bunny   | [SEP]    |\n",
      "| after   | [SEP]    |\n",
      "| a       | [SEP]    |\n",
      "| long    | .        |\n",
      "| [SEP]   | .        |\n",
      "+---------+----------+\n"
     ]
    }
   ],
   "source": [
    "for batch_norm in batch_norm_params:\n",
    "    print(f'batch_norm = {batch_norm}')\n",
    "    model = Autoencoder(vocab_dim=len(word2idx), emb_dim=10, latent_dim=10, hidden_dim=10, batch_norm=batch_norm)\n",
    "    model.to(device)\n",
    "    writer = SummaryWriter(log_dir=f'tensorboard3/batch_norm_{batch_norm}')\n",
    "    call = callback(writer, dataset_test_pt, loss_function)\n",
    "    check_model(64, dataset_test_pt, model, loss_function, idx2word)\n",
    "    trainer(count_of_epoch=1,\n",
    "            batch_size=64,\n",
    "            dataset=dataset_train_pt,\n",
    "            model=model,\n",
    "            loss_function=loss_function,\n",
    "            optimizer = optimizer,\n",
    "            lr=0.001,\n",
    "            callback = call)\n",
    "    check_model(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26447f8-c64d-4729-b1ee-0440d3ad22d8",
   "metadata": {},
   "source": [
    "Уменьшение размера словаря:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f1f5f0b-f1b1-4cec-8c3a-480493bcaec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_count = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2217f3b04f1d43fa9be2703a488d3b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 11.510943992614745\n",
      "+-------+---------+\n",
      "| Word  | Predict |\n",
      "+-------+---------+\n",
      "| [CLS] | jhdrr   |\n",
      "| #     | jhdrr   |\n",
      "| [UNK] | jhdrr   |\n",
      "| #     | jhdrr   |\n",
      "| [UNK] | jhdrr   |\n",
      "| 09    | jhdrr   |\n",
      "| 35    | jhdrr   |\n",
      "| .     | jhdrr   |\n",
      "| [UNK] | jhdrr   |\n",
      "| 14    | jhdrr   |\n",
      "| .     | jhdrr   |\n",
      "| [SEP] | jhdrr   |\n",
      "+-------+---------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a42d90bb8b49088976c0345f991363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=7.440906047821045, val_loss=7.312699252319336\n",
      "\t\tstep=600, train_loss=6.278777599334717, val_loss=6.365197151947021\n",
      "\t\tstep=900, train_loss=6.1684112548828125, val_loss=6.235853273773193\n",
      "\t\tstep=1200, train_loss=6.075265407562256, val_loss=6.152376276397705\n",
      "loss: 6.139908186340332\n",
      "+---------+---------+\n",
      "| Word    | Predict |\n",
      "+---------+---------+\n",
      "| [CLS]   | [CLS]   |\n",
      "| Well    | @       |\n",
      "| I       | [SEP]   |\n",
      "| said    | [SEP]   |\n",
      "| I       | [SEP]   |\n",
      "| would   | [SEP]   |\n",
      "| drag    | [SEP]   |\n",
      "| the     | [SEP]   |\n",
      "| fine    | [SEP]   |\n",
      "| weather | [SEP]   |\n",
      "| north   | [SEP]   |\n",
      "| [SEP]   | [SEP]   |\n",
      "+---------+---------+\n",
      "min_count = 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ac0a3eb3df4371a15f302a1b29d9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9.804461152648926\n",
      "+---------+---------+\n",
      "| Word    | Predict |\n",
      "+---------+---------+\n",
      "| [CLS]   | pickle  |\n",
      "| ALWAYS  | pickle  |\n",
      "| LOOKING | PEOPLE  |\n",
      "| FOR     | PEOPLE  |\n",
      "| [UNK]   | PEOPLE  |\n",
      "| AND     | PEOPLE  |\n",
      "| [UNK]   | jst     |\n",
      "| [UNK]   | jst     |\n",
      "| ,       | jst     |\n",
      "| IF      | jst     |\n",
      "| [UNK]   | jst     |\n",
      "| [SEP]   | jst     |\n",
      "+---------+---------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3c083626e84d7a994a4e12b28748e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=6.16902494430542, val_loss=6.088178916931152\n",
      "\t\tstep=600, train_loss=5.598550796508789, val_loss=5.530441246795654\n",
      "\t\tstep=900, train_loss=5.373997211456299, val_loss=5.438145085144043\n",
      "\t\tstep=1200, train_loss=5.326599597930908, val_loss=5.389489937591553\n",
      "loss: 5.3827854225158696\n",
      "+-----------+---------+\n",
      "| Word      | Predict |\n",
      "+-----------+---------+\n",
      "| [CLS]     | [CLS]   |\n",
      "| Currently | [CLS]   |\n",
      "| doing     | [SEP]   |\n",
      "| research  | [SEP]   |\n",
      "| for       | [SEP]   |\n",
      "| a         | [SEP]   |\n",
      "| school    | [SEP]   |\n",
      "| project   | [SEP]   |\n",
      "| Its       | [SEP]   |\n",
      "| due       | [SEP]   |\n",
      "| wednesday | [SEP]   |\n",
      "| [SEP]     | [SEP]   |\n",
      "+-----------+---------+\n",
      "min_count = 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55dc94edb8a24862a11cd2ba361edca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9.40808381652832\n",
      "+-------+---------+\n",
      "| Word  | Predict |\n",
      "+-------+---------+\n",
      "| [CLS] | green   |\n",
      "| @     | GO      |\n",
      "| [UNK] | GO      |\n",
      "| OK    | GO      |\n",
      "| -     | GO      |\n",
      "| gonna | GO      |\n",
      "| give  | ie      |\n",
      "| it    | ie      |\n",
      "| a     | ie      |\n",
      "| try   | ie      |\n",
      "| on    | ie      |\n",
      "| [SEP] | ie      |\n",
      "+-------+---------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1d75b0f35c445d97782a1e05839312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=6.031227111816406, val_loss=5.978123084259034\n",
      "\t\tstep=600, train_loss=5.399112701416016, val_loss=5.2325722648620605\n",
      "\t\tstep=900, train_loss=5.28649377822876, val_loss=5.061709470367432\n",
      "\t\tstep=1200, train_loss=4.977850914001465, val_loss=4.980846421813965\n",
      "loss: 4.969283097076416\n",
      "+----------+---------+\n",
      "| Word     | Predict |\n",
      "+----------+---------+\n",
      "| [CLS]    | [CLS]   |\n",
      "| @        | @       |\n",
      "| [UNK]    | [UNK]   |\n",
      "| 77       | [UNK]   |\n",
      "| thats    | [UNK]   |\n",
      "| great    | [UNK]   |\n",
      "| !        | [UNK]   |\n",
      "| !        | [UNK]   |\n",
      "| !        | [UNK]   |\n",
      "| whatever | [UNK]   |\n",
      "| the      | [SEP]   |\n",
      "| [SEP]    | [SEP]   |\n",
      "+----------+---------+\n"
     ]
    }
   ],
   "source": [
    "for min_count in min_count_params:\n",
    "    print(f'min_count = {min_count}')\n",
    "    word2idx, idx2word = word_dict(dataset_train, min_count=min_count)\n",
    "    tokenizer = Tokenizer(word2idx, RegexpTokenizer('[a-zA-Z]+|[^\\w\\s]|\\d+'))\n",
    "    train_data_sent = tokenizer(dataset_train.values[:, 1])\n",
    "    test_data_sent = tokenizer(dataset_test.values[:, 1])\n",
    "    dataset_train_pt = TensorDataset(train_data_sent, train_data_sent)\n",
    "    dataset_test_pt = TensorDataset(test_data_sent, test_data_sent)\n",
    "    model = Autoencoder(vocab_dim=len(word2idx), emb_dim=10, latent_dim=10, hidden_dim=10)\n",
    "    model.to(device)\n",
    "    writer = SummaryWriter(log_dir=f'tensorboard3/min_count_{min_count}')\n",
    "    call = callback(writer, dataset_test_pt, loss_function)\n",
    "    check_model(64, dataset_test_pt, model, loss_function, idx2word)\n",
    "    trainer(count_of_epoch=1,\n",
    "            batch_size=64,\n",
    "            dataset=dataset_train_pt,\n",
    "            model=model,\n",
    "            loss_function=loss_function,\n",
    "            optimizer = optimizer,\n",
    "            lr=0.001,\n",
    "            callback = call)\n",
    "    check_model(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1daf66b-2a43-4a56-9f35-0460c6822d70",
   "metadata": {},
   "source": [
    "## Обучение модели с наилучшими параметрами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca962d6-50fa-41f5-b375-c7b336dead79",
   "metadata": {},
   "source": [
    "После анализов результатов (в т.ч. графиков tensorboard) можно сделать вывод, что лучше выбирать модель с большим количеством и размером слоёв, dropout можно сказать не влияет на результат, batchnorm дал странный результат (похоже на переобучение, очень плохие результаты на тесте и даже на трейне сначала, потом разница небольшая), редко используемые слова лучше убрать из словаря. Протестируем модель с наибольшим (из экспериментов) размером и количеством слоёв, выключенным dropout и batchnorm, наименьшим размером словаря."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "896cc265-1a5b-4f36-9287-2aeb20e68561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656fe099f9524d5ea7f36cdda7e569a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9.392846656799316\n",
      "+-------+---------+\n",
      "| Word  | Predict |\n",
      "+-------+---------+\n",
      "| [CLS] | doctors |\n",
      "| haha  | tot     |\n",
      "| ,     | tot     |\n",
      "| didn  | tot     |\n",
      "| '     | tot     |\n",
      "| t     | tot     |\n",
      "| post  | demon   |\n",
      "| in    | demon   |\n",
      "| a     | demon   |\n",
      "| long  | demon   |\n",
      "| time  | demon   |\n",
      "| [SEP] | demon   |\n",
      "+-------+---------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e646c53c469c49acbd34310742e8cd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=5.054401874542236, val_loss=4.968156016540528\n",
      "\t\tstep=600, train_loss=4.913304805755615, val_loss=4.8078180587768555\n",
      "\t\tstep=900, train_loss=4.804904460906982, val_loss=4.740937191009522\n",
      "\t\tstep=1200, train_loss=4.738305568695068, val_loss=4.692846277618409\n",
      "loss: 4.681375462341308\n",
      "+---------+---------+\n",
      "| Word    | Predict |\n",
      "+---------+---------+\n",
      "| [CLS]   | [CLS]   |\n",
      "| yeah    | I       |\n",
      "| ,       | I       |\n",
      "| its     | .       |\n",
      "| the     | .       |\n",
      "| weekend | .       |\n",
      "| [SEP]   | .       |\n",
      "+---------+---------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.681375462341308"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx, idx2word = word_dict(dataset_train, min_count = 5)\n",
    "tokenizer = Tokenizer(word2idx, RegexpTokenizer('[a-zA-Z]+|[^\\w\\s]|\\d+'))\n",
    "train_data_sent = tokenizer(dataset_train.values[:, 1])\n",
    "test_data_sent = tokenizer(dataset_test.values[:, 1])\n",
    "dataset_train_pt = TensorDataset(train_data_sent, train_data_sent)\n",
    "dataset_test_pt = TensorDataset(test_data_sent, test_data_sent)\n",
    "model = Autoencoder(vocab_dim=len(word2idx), emb_dim = 50, latent_dim = 50, hidden_dim = 50, num_layers = 7, dropout = 0, batch_norm = False)\n",
    "model.to(device)\n",
    "writer = SummaryWriter(log_dir=f'tensorboard3/final')\n",
    "call = callback(writer, dataset_test_pt, loss_function)\n",
    "check_model(64, dataset_test_pt, model, loss_function, idx2word)\n",
    "trainer(count_of_epoch=1,\n",
    "        batch_size=64,\n",
    "        dataset=dataset_train_pt,\n",
    "        model=model,\n",
    "        loss_function=loss_function,\n",
    "        optimizer = optimizer,\n",
    "        lr=0.001,\n",
    "        callback = call)\n",
    "check_model(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729823df-62c9-4e88-92b3-69b597add525",
   "metadata": {},
   "source": [
    "Да, эта модель дала абсолютно лучший результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b931abb-454d-47dc-8007-44f9f3589429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_with_acc(batch_size, dataset, model, loss_function, idx2word):\n",
    "    model.eval()\n",
    "    batch_generator = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "    test_loss = 0\n",
    "    total_correct = 0\n",
    "    total_words = 0\n",
    "    for it, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "        x_batch = x_batch.to(model.device)\n",
    "        y_batch = y_batch.to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output = model(x_batch)\n",
    "        test_loss += loss_function(output.transpose(1, 2), y_batch).cpu().item() * len(x_batch)\n",
    "        # Получаем предсказанные слова\n",
    "        predicted_indices = output.argmax(dim=-1)  # Индексы предсказанных слов\n",
    "        total_correct += (predicted_indices == y_batch).sum().item()  # Считаем правильные предсказания\n",
    "        total_words += y_batch.numel()  # Общее количество слов в батче\n",
    "    test_loss /= len(dataset)\n",
    "    accuracy = total_correct / total_words  # Вычисляем accuracy\n",
    "    print(f'loss: {test_loss}, accuracy: {accuracy:.4f}')  # Выводим loss и accuracy\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    x, y = next(iter(dataloader))\n",
    "    x = x.to(model.device)\n",
    "    y = y.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x)\n",
    "    one_x = x[0].cpu().numpy()\n",
    "    one_output = outputs[0].argmax(dim=-1).cpu().numpy()\n",
    "    words = [idx2word[idx] for idx in one_x]\n",
    "    pred_words = [idx2word[idx] for idx in one_output]\n",
    "    table = PrettyTable([\"Word\", \"Predict\"])\n",
    "    table.align[\"Word\"], table.align[\"Predict\"] = \"l\", \"l\"\n",
    "    for word, pred in zip(words, pred_words):\n",
    "        if word != idx2word[word2idx['[PAD]']]:\n",
    "            table.add_row([word, pred])\n",
    "    print(table)\n",
    "    return test_loss, accuracy  # Возвращаем также accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40ac7f1-4b68-41e3-9706-da92ef0aa8a4",
   "metadata": {},
   "source": [
    "Однако качество восстановления всё равно плохое. Скорее всего, для данной задачи нужно больше эпох при обучении и, возможно, больше исходных данных. А также имеет смысл попробовать ещё увеличить размер слоя. Но и так каждая модель обучалась где-то 40-45 минут, это долго (хотя всего 1 эпоха)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78c87e58-d491-4ec4-97d3-c9f1fab906c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.681375462341308, accuracy: 0.2809\n",
      "+-------+---------+\n",
      "| Word  | Predict |\n",
      "+-------+---------+\n",
      "| [CLS] | [CLS]   |\n",
      "| I     | I       |\n",
      "| hate  | '       |\n",
      "| this  | .       |\n",
      "| part  | .       |\n",
      "| right | .       |\n",
      "| here  | .       |\n",
      "| !     | .       |\n",
      "| I     | .       |\n",
      "| hate  | .       |\n",
      "| the   | [SEP]   |\n",
      "| [SEP] | [SEP]   |\n",
      "+-------+---------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.681375462341308, 0.28086666666666665)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_model_with_acc(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d2126d-2291-4cd1-b8cb-765126d506b4",
   "metadata": {},
   "source": [
    "Хотя при уменьшении размера словаря обучение сильно ускорилось. Дадим ещё один шанс модели, увеличим размер слоя и число эпох обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5fcc913-ae2d-4087-935e-c050a9bfb791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9.391493713378907, accuracy: 0.0000\n",
      "+----------+-----------+\n",
      "| Word     | Predict   |\n",
      "+----------+-----------+\n",
      "| [CLS]    | possible  |\n",
      "| I        | microsoft |\n",
      "| was      | microsoft |\n",
      "| thinking | microsoft |\n",
      "| to       | microsoft |\n",
      "| myself   | microsoft |\n",
      "| &        | microsoft |\n",
      "| quot     | microsoft |\n",
      "| ;        | microsoft |\n",
      "| wow      | microsoft |\n",
      "| what     | microsoft |\n",
      "| [SEP]    | microsoft |\n",
      "+----------+-----------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4cab3df9f4402882b923c5646d0970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=300, train_loss=5.027698516845703, val_loss=4.920947412109375\n",
      "\t\tstep=600, train_loss=4.9747161865234375, val_loss=4.865446396636963\n",
      "\t\tstep=900, train_loss=4.737959384918213, val_loss=4.84345210723877\n",
      "\t\tstep=1200, train_loss=4.845438003540039, val_loss=4.831939554595947\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=1500, train_loss=4.881037712097168, val_loss=4.735683513641358\n",
      "\t\tstep=1800, train_loss=4.7061944007873535, val_loss=4.694440692138672\n",
      "\t\tstep=2100, train_loss=4.5958428382873535, val_loss=4.6776361106872555\n",
      "\t\tstep=2400, train_loss=4.552255153656006, val_loss=4.665281605529785\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=2700, train_loss=4.675617218017578, val_loss=4.632002725982666\n",
      "\t\tstep=3000, train_loss=4.673832893371582, val_loss=4.60349369430542\n",
      "\t\tstep=3300, train_loss=4.731934070587158, val_loss=4.576621356201172\n",
      "\t\tstep=3600, train_loss=4.607517242431641, val_loss=4.540748796844483\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=3900, train_loss=4.375320911407471, val_loss=4.5430634971618655\n",
      "\t\tstep=4200, train_loss=4.420837879180908, val_loss=4.507258138275146\n",
      "\t\tstep=4500, train_loss=4.59522819519043, val_loss=4.493655236816406\n",
      "\t\tstep=4800, train_loss=4.453329086303711, val_loss=4.483151457214356\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tstep=5100, train_loss=4.647826671600342, val_loss=4.460988728332519\n",
      "\t\tstep=5400, train_loss=4.558731555938721, val_loss=4.447318857574463\n",
      "\t\tstep=5700, train_loss=4.181212902069092, val_loss=4.425877815246582\n",
      "\t\tstep=6000, train_loss=4.449232578277588, val_loss=4.399534207916259\n",
      "loss: 4.389816265106202, accuracy: 0.3069\n",
      "+---------+---------+\n",
      "| Word    | Predict |\n",
      "+---------+---------+\n",
      "| [CLS]   | [CLS]   |\n",
      "| [UNK]   | [UNK]   |\n",
      "| -       | [UNK]   |\n",
      "| Twitter | [UNK]   |\n",
      "| for     | .       |\n",
      "| [UNK]   | .       |\n",
      "| users   | .       |\n",
      "| ,       | .       |\n",
      "| [UNK]   | .       |\n",
      "| the     | .       |\n",
      "| spam    | ,       |\n",
      "| [SEP]   | [SEP]   |\n",
      "+---------+---------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.389816265106202, 0.3069375)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Autoencoder(vocab_dim=len(word2idx), emb_dim = 100, latent_dim = 100, hidden_dim = 100, num_layers = 7, dropout = 0, batch_norm = False)\n",
    "model.to(device)\n",
    "writer = SummaryWriter(log_dir=f'tensorboard3/cheat')\n",
    "call = callback(writer, dataset_test_pt, loss_function)\n",
    "check_model_with_acc(64, dataset_test_pt, model, loss_function, idx2word)\n",
    "trainer(count_of_epoch=5,\n",
    "        batch_size=64,\n",
    "        dataset=dataset_train_pt,\n",
    "        model=model,\n",
    "        loss_function=loss_function,\n",
    "        optimizer = optimizer,\n",
    "        lr=0.001,\n",
    "        callback = call)\n",
    "check_model_with_acc(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32478900-eba4-4b1e-be8c-8175baa816ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.389816265106202, accuracy: 0.3069\n",
      "+-----------+---------+\n",
      "| Word      | Predict |\n",
      "+-----------+---------+\n",
      "| [CLS]     | [CLS]   |\n",
      "| I         | I       |\n",
      "| have      | am      |\n",
      "| only      | [UNK]   |\n",
      "| two       | [UNK]   |\n",
      "| (         | .       |\n",
      "| 2         | .       |\n",
      "| )         | .       |\n",
      "| followers | .       |\n",
      "| .         | .       |\n",
      "| .         | .       |\n",
      "| [SEP]     | [SEP]   |\n",
      "+-----------+---------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.389816265106202, 0.3069375)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_model_with_acc(64, dataset_test_pt, model, loss_function, idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb5a48-4f24-428a-a676-34668266cf8d",
   "metadata": {},
   "source": [
    "К сожалению, результат всё же плохой, модель в основном выдаёт знаки препинания. Но что-то получается восстановить (I am вместо I have). Зависимость качества от параметров проследить удалось, странный только результат при batchnorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3a7d6-89ba-4fa8-8fea-bd7fea055d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
